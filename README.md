# Robust-Generative-AI-Adversarial-Attack-Defense-on-LLMs-
Robust Generative AI– Adversarial Attack &amp; Defense on LLMs  AI Security– Implement FGSM, PGD, and C&amp;W attacks to manipulate text generation in GPT-2– Apply adversarial training and defensive distillation to enhance model robustness– Compare attack success rates and defense effectiveness

![Python Version](https://img.shields.io/badge/python-3.7%2B-blue)
![PyTorch Version](https://img.shields.io/badge/pytorch-1.8%2B-orange)
![HuggingFace Transformers](https://img.shields.io/badge/transformers-4.0%2B-yellowgreen)

This project demonstrates adversarial attacks and defense strategies on GPT-2 language model, focusing on text generation robustness.

## Features

- **Three Attack Methods**:
  - FGSM (Fast Gradient Sign Method)
  - PGD (Projected Gradient Descent)
  - C&W (Carlini & Wagner)

- **Two Defense Strategies**:
  - Adversarial Training
  - Defensive Distillation

- **Evaluation Framework**:
  - Attack success rates
  - Perplexity changes
  - Defense effectiveness

## Installation

1. Clone the repository:
```bash
git clone https://github.com/yourusername/robust-generative-ai.git
cd robust-generative-ai
